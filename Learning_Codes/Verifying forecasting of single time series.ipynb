{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7a03bd",
   "metadata": {},
   "source": [
    "### Verifying the forecasting of a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6206d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully\n"
     ]
    }
   ],
   "source": [
    "## Fault Injection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "# from random import random\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# def randomised(data):\n",
    "#     n = len(data)\n",
    "#     fault_units = [-30.5, -20.5, 0, 15.5, 25.5]\n",
    "#     for col in data.columns:\n",
    "        \n",
    "#         for i in range(len(data)):\n",
    "#             p = random.random()\n",
    "#             if p > 0.5:\n",
    "#                 data[col].iloc[i] += random.choice(fault_units)\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "def value_flip(data, loc, col):\n",
    "    for dp in data.index[loc]:\n",
    "        if(data.loc[dp, col] == 1):\n",
    "            data.loc[dp, col] = 0\n",
    "        else:\n",
    "            data.loc[dp, col] = 1            \n",
    "    return data\n",
    "\n",
    "\n",
    "def stuck_at(data, loc, col, val):\n",
    "    # This is either stuck_at zero or at 1\n",
    "    for dp in data.index[loc]:\n",
    "        data.loc[dp, col] = val\n",
    "    return data\n",
    "\n",
    "def insert_outlier(data, loc, col):\n",
    "#     // This inserts at the beginning of the data\n",
    "    thresh = random.uniform(0.5, 1.5)\n",
    "    q1 = np.quantile(data, 0.25)\n",
    "    q3 = np.quantile(data, 0.75)\n",
    "    iqr = q3 - q1\n",
    "    high_outlier = q3 + 1.5*iqr\n",
    "    low_outlier = q1 - 1.5*iqr - thresh\n",
    "    for dp in data.index[loc]:\n",
    "        data.loc[dp, col] =high_outlier + random.uniform(0.5, 1.5)\n",
    "#         data.loc[dp, col] =low_outlier - random.uniform(0.5, 1.5)\n",
    "    return data\n",
    "\n",
    "\n",
    "def stuck2_at(data, val, col, loc=\"beg\"):\n",
    "    # This is either stuck_at zero or at 1\n",
    "    if(loc == \"beg\"):\n",
    "        n = math.floor(len(data[col])*0.3)\n",
    "        for i in range(n):\n",
    "            data[col][i] = val\n",
    "    return data\n",
    "\n",
    "# mydata = read_csv('simulated_waterTank.csv', header=0, index_col=0, skiprows=4)\n",
    "mydata = read_csv('waterTankNew.csv', header=0, index_col=0)\n",
    "\n",
    "faulty_data = stuck_at(mydata, range(500,700), \"Tank1InFlow\", 1)\n",
    "faulty_data.to_csv('stuck_at_faulty_data.csv')\n",
    "\n",
    "faulty_data = stuck_at(mydata, range(500,700), \"Tank1InFlow\", 0)\n",
    "faulty_data.to_csv('stuck_at_zero_faulty_data.csv')\n",
    "\n",
    "\n",
    "faulty_data_vf = value_flip(mydata, range(500,700), \"Tank1InFlow\")\n",
    "faulty_data_vf.to_csv('value_flip_faulty_data.csv')\n",
    "\n",
    "faulty_data_outlier = insert_outlier(mydata, range(500,700), \"Tank1InFlow\")\n",
    "faulty_data_outlier.to_csv('outlier_faulty_data.csv')\n",
    "# faulty_data.head()\n",
    "# mydata\n",
    "\n",
    "print(\"Files saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad743b",
   "metadata": {},
   "source": [
    "### Prediction with faulty data (stuck_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b97def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the raw_data before scaling is (20001, 5)\n",
      "loss:  7.28%\n",
      "Prediction is successful!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "s_model = load_model('Enc-Dec-lstm.h5')\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "data = read_csv('stuck_at_faulty_data.csv', header=0, index_col=0)\n",
    "\n",
    "# horizontally stack columns\n",
    "raw_data = data.values\n",
    "print(f'The shape of the raw_data before scaling is {raw_data.shape}')\n",
    "#normalize input features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "f_scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "# n_train = 10000\n",
    "# dataset = f_scaled_data[0:n_train,:]\n",
    "\n",
    "\n",
    "new_data = f_scaled_data[0:2000,:]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 50, 100\n",
    "\n",
    "# convert into input/output\n",
    "test_X, test_y = split_sequences(new_data, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = test_X.shape[2]\n",
    "\n",
    "score = s_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_yhat = s_model.predict(test_X, verbose=0)\n",
    "print(\"%s:  %.2f%%\" % (s_model.metrics_names[0], score*100))\n",
    "\n",
    "print(\"Prediction is successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93ab39",
   "metadata": {},
   "source": [
    "### Prediction with faulty data (value_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc46bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the raw_data before scaling is (20001, 5)\n",
      "loss:  7.60%\n",
      "Prediction is successful!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "s_model = load_model('Enc-Dec-lstm.h5')\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "data = read_csv('value_flip_faulty_data.csv', header=0, index_col=0)\n",
    "\n",
    "# horizontally stack columns\n",
    "raw_data = data.values\n",
    "print(f'The shape of the raw_data before scaling is {raw_data.shape}')\n",
    "#normalize input features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "f_scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "# n_train = 10000\n",
    "# dataset = f_scaled_data[0:n_train,:]\n",
    "\n",
    "\n",
    "new_data = f_scaled_data[0:2000,:]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 50, 100\n",
    "\n",
    "# convert into input/output\n",
    "test_X, test_y = split_sequences(new_data, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = test_X.shape[2]\n",
    "\n",
    "score = s_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_yhat = s_model.predict(test_X, verbose=0)\n",
    "print(\"%s:  %.2f%%\" % (s_model.metrics_names[0], score*100))\n",
    "\n",
    "print(\"Prediction is successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48007700",
   "metadata": {},
   "source": [
    "### Prediction with faulty data (Outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea5823c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the raw_data before scaling is (20001, 5)\n",
      "loss:  6.88%\n",
      "Prediction is successful!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "s_model = load_model('Enc-Dec-lstm.h5')\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "data = read_csv('outlier_faulty_data.csv', header=0, index_col=0)\n",
    "\n",
    "# horizontally stack columns\n",
    "raw_data = data.values\n",
    "print(f'The shape of the raw_data before scaling is {raw_data.shape}')\n",
    "#normalize input features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "f_scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "# n_train = 10000\n",
    "# dataset = f_scaled_data[0:n_train,:]\n",
    "\n",
    "\n",
    "new_data = f_scaled_data[0:2000,:]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 50, 100\n",
    "\n",
    "# convert into input/output\n",
    "test_X, test_y = split_sequences(new_data, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = test_X.shape[2]\n",
    "\n",
    "score = s_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_yhat = s_model.predict(test_X, verbose=0)\n",
    "print(\"%s:  %.2f%%\" % (s_model.metrics_names[0], score*100))\n",
    "\n",
    "print(\"Prediction is successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29fa15",
   "metadata": {},
   "source": [
    "### Prediction with faulty data (Outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c9a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the raw_data before scaling is (20001, 5)\n",
      "loss:  7.60%\n",
      "Prediction is successful!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "s_model = load_model('Enc-Dec-lstm.h5')\n",
    "\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "data = read_csv('stuck_at_zero_faulty_data.csv', header=0, index_col=0)\n",
    "\n",
    "# horizontally stack columns\n",
    "raw_data = data.values\n",
    "print(f'The shape of the raw_data before scaling is {raw_data.shape}')\n",
    "#normalize input features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "f_scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "# n_train = 10000\n",
    "# dataset = f_scaled_data[0:n_train,:]\n",
    "\n",
    "\n",
    "new_data = f_scaled_data[0:2000,:]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 50, 100\n",
    "\n",
    "# convert into input/output\n",
    "test_X, test_y = split_sequences(new_data, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = test_X.shape[2]\n",
    "\n",
    "score = s_model.evaluate(test_X, test_y, verbose=0)\n",
    "test_yhat = s_model.predict(test_X, verbose=0)\n",
    "print(\"%s:  %.2f%%\" % (s_model.metrics_names[0], score*100))\n",
    "\n",
    "print(\"Prediction is successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ab3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701f51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1aedd78",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526e024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
